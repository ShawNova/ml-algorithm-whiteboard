{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is openai api?\n",
    "\n",
    "The OpenAI API provides a simple interface to state-of-the-art AI models for natural language processing, image generation, semantic search, and speech recognition. The best practices of OpenAI API is to generate human-like responses to natural language prompts, create vector embeddings for semantic search, and generate images from textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model_name = os.getenv('OPENAI_MODEL')\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [text generation](https://platform.openai.com/docs/guides/text-generation)\n",
    "\n",
    "We use openai.chat.completions endpoint in the REST API / OpenAI SDKs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"best\" way to make a cup of coffee can vary greatly depending on personal preference, as well as the type of coffee you enjoy. However, one popular and widely appreciated method is the pour-over technique, known for producing a clean and flavorful cup. Hereâ€™s a simple guide to making coffee using the pour-over method:\n",
      "\n",
      "### Equipment and Ingredients Needed:\n",
      "- Freshly roasted coffee beans\n",
      "- Burr grinder (to ensure even grind size)\n",
      "- Kettle (preferably with a gooseneck spout for precision)\n",
      "- Pour-over dripper (e.g., Hario V60, Kalita Wave, or Chemex)\n",
      "- Paper filter (choose one that fits your dripper)\n",
      "- Digital kitchen scale\n",
      "- Timer\n",
      "- Mug\n"
     ]
    }
   ],
   "source": [
    "# example prompt\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the best way to make a cup of coffee?\"},\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    max_tokens=150,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "message usage\n",
    "\n",
    "a message consists of lists of dict, where each dict is a message including role and content.\n",
    "\n",
    "- content: string or array\n",
    "```python\n",
    "# single content\n",
    "{\"role\": \"user\", \"content\": \"count down from 10\"}\n",
    "# multiple content\n",
    "{\n",
    "    \"role\": \"user\", \n",
    "    \"content\": [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"content\": \"what's in the picture?\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"content\": \"https://***.jpg\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "- role\n",
    "\n",
    "| role | description |\n",
    "|---|---|\n",
    "|user| similar to end user in the webui/front-end|\n",
    "|developer(system)| Instructions to the model that are prioritized ahead of user messages, following chain of command. Previously called the system prompt. |\n",
    "|assistant| A message generated by the model, perhaps in a previous generation request. usually in a conversation|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multimodality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### images\n",
    "\n",
    "#### image2text\n",
    "By combining ser prompts and image content(everything should be json-like), we could ask about the questions like what is in the picture with low/high/auto fidelity. We could also upload local images with ```Base64``` encoded in the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"image_url\",\n",
    "                        \"detail\": \"high\", # optional\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"image_path\", \"detail\": \"auto\"},\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## structure output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embeddings\n",
    "\n",
    "Getting embeddings with openai embeddings API endpoint along with the embedding model name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fundemental usage of embeddings\n",
    "\n",
    "1. use client.embeddings to generate a query for the embedding with given corpus\n",
    "2. the query could be used combining dataframe function and schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "embedding_model = \"text-embedding-3-small\"\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=embedding_model,\n",
    "    input=\"Sample document\",\n",
    "    dimensions=10  # optional, defaults 1536 for text-embedding-3-small and 3072 for text-embedding-3-large\n",
    ")\n",
    "\n",
    "print(len(response.data[0].embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### usage of embeddings\n",
    "\n",
    "1.  embedding-based search\n",
    "2.  text/code/recommendation with search: similarity measurement with cosine similarity\n",
    "3.  zero-shot learning\n",
    "4.  cold-start solving\n",
    "5.  clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json structure output\n",
    "\n",
    "benefits:\n",
    "1. simpler prompting: no need to handcraft complex prompts\n",
    "2. consistency: no need to validate format\n",
    "\n",
    "usage:\n",
    "1. formatted output\n",
    "2. chain of thoughts\n",
    "\n",
    "To turn on JSON mode with the Chat Completions or Assistants API you can set the response_format to { \"type\": \"json_object\" }. If you are using function calling, JSON mode is always turned on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel # pydantic is a library for python data validation. often used to define schemas.\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predicted outputs\n",
    "\n",
    "Reduce latency for model responses where much of the response is known ahead of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-B7PyT1H43QdZbNHEXvBF88p752b45', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='class User {\\n  firstName: string = \"\";\\n  lastName: string = \"\";\\n  email: string = \"\";\\n}\\n\\nexport default User;', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1741107841, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_f9f4fb6dbf', usage=CompletionUsage(completion_tokens=45, prompt_tokens=66, total_tokens=111, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=16), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "class User {\n",
      "  firstName: string = \"\";\n",
      "  lastName: string = \"\";\n",
      "  email: string = \"\";\n",
      "}\n",
      "\n",
      "export default User;\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "code = \"\"\"\n",
    "class User {\n",
    "  firstName: string = \"\";\n",
    "  lastName: string = \"\";\n",
    "  username: string = \"\";\n",
    "}\n",
    "\n",
    "export default User;\n",
    "\"\"\"\n",
    "\n",
    "refactor_prompt = \"\"\"\n",
    "Replace the \"username\" property with an \"email\" property. Respond only\n",
    "with code, and with no markdown formatting.\n",
    "\"\"\"\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": refactor_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": code\n",
    "        }\n",
    "    ],\n",
    "    prediction={\n",
    "        \"type\": \"content\",\n",
    "        \"content\": code\n",
    "    }\n",
    ")\n",
    "\n",
    "print(completion)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Function calling](https://platform.openai.com/docs/guides/function-calling)\n",
    "\n",
    "Extending openai model by giving access to ```tools```:\n",
    "\n",
    "|type| description|\n",
    "|---|---|\n",
    "|Function Calling| Developer-defined tools|\n",
    "|Hosted Tools| OpenAI SDK built-in tools(file search, code interpreter) with assistant API|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Reasoning with test-time compute](https://platform.openai.com/docs/guides/reasoning)\n",
    "\n",
    "The reasoning takes intermediate reasoning tokens in addition to the input and output tokens. The intermediate tokens are where the balance between reasoning performance and reasoning cost is. It breaks down the reasoning into several reasoning steps with the RL fine-tuning. The cost is controllable. Reasoning mode may provide better results on high-level prompts than precise instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To transpose a matrix represented as a string in the format `[1,2],[3,4],[5,6]`, you can use a bash script that processes the string, transposes the rows and columns, and then outputs the transposed matrix in the same format. Below is a sample bash script to achieve this:\n",
      "\n",
      "```bash\n",
      "#!/bin/bash\n",
      "\n",
      "# Function to parse the input string into an array of arrays (matrix)\n",
      "parse_matrix() {\n",
      "    local input=\"$1\"\n",
      "    input=\"${input#[}\"  # Remove the leading '['\n",
      "    input=\"${input%]}\"  # Remove the trailing ']'\n",
      "\n",
      "    # Extract each row and add it to the matrix array\n",
      "    local matrix=()\n",
      "    IFS=',' read -r -a rows <<< \"${input//],/[}\"\n",
      "    for ((i = 0; i < ${#rows[@]}; i+=2)); do\n",
      "        matrix+=(\"(${rows[i]},${rows[i+1]})\")\n",
      "    done\n",
      "\n",
      "    echo \"${matrix[@]}\"\n",
      "}\n",
      "\n",
      "# Function to transpose the matrix\n",
      "transpose_matrix() {\n",
      "    local matrix=(\"$@\")\n",
      "    local transposed_matrix=()\n",
      "\n",
      "    # Calculate number of rows and columns\n",
      "    local num_rows=${#matrix[@]}\n",
      "    local num_cols=$(echo \"${matrix[0]}\" | tr -cd ',' | wc -c)\n",
      "    ((num_cols++))\n",
      "\n",
      "    # Initialize transposed matrix\n",
      "    for ((i = 0; i < num_cols; i++)); do\n",
      "        transposed_matrix[i]=\"\"\n",
      "    done\n",
      "\n",
      "    # Fill the transposed matrix\n",
      "    for ((i = 0; i < num_rows; i++)); do\n",
      "        local row=\"${matrix[i]}\"\n",
      "        row=\"${row#[}\"\n",
      "        row=\"${row%]}\"\n",
      "        local cols=()\n",
      "        IFS=',' read -r -a cols <<< \"${row}\"\n",
      "        for ((j = 0; j < num_cols; j++)); do\n",
      "            transposed_matrix[j]+=\"${cols[j]},\"\n",
      "        done\n",
      "    done\n",
      "\n",
      "    # Format transposed matrix correctly\n",
      "    local result=\"\"\n",
      "    for ((i = 0; i < num_cols; i++)); do\n",
      "        local t_row=\"${transposed_matrix[i]}\"\n",
      "        t_row=\"[${t_row%,}]\"\n",
      "        result+=\"${t_row},\"\n",
      "    done\n",
      "\n",
      "    # Remove the trailing comma\n",
      "    result=\"${result%,}\"\n",
      "\n",
      "    echo \"$result\"\n",
      "}\n",
      "\n",
      "# The main function to execute the script logic\n",
      "main() {\n",
      "    if [[ -z \"$1\" ]]; then\n",
      "        echo \"Usage: $0 \\\"[1,2],[3,4],[5,6]\\\"\"\n",
      "        exit 1\n",
      "    fi\n",
      "\n",
      "    # Parse and transpose the matrix\n",
      "    matrix=$(parse_matrix \"$1\")\n",
      "    transposed=$(transpose_matrix \"${matrix[@]}\")\n",
      "    echo \"$transposed\"\n",
      "}\n",
      "\n",
      "# Execute the main function passing all script arguments\n",
      "main \"$@\"\n",
      "```\n",
      "\n",
      "### Usage\n",
      "Save this script to a file, for example `transpose_matrix.sh`, and make it executable:\n",
      "\n",
      "```bash\n",
      "chmod +x transpose_matrix.sh\n",
      "```\n",
      "\n",
      "You can run this script with the following command:\n",
      "\n",
      "```bash\n",
      "./transpose_matrix.sh \"[1,2],[3,4],[5,6]\"\n",
      "```\n",
      "\n",
      "This will output the transposed matrix:\n",
      "\n",
      "```plaintext\n",
      "[1,3,5],[2,4,6]\n",
      "```\n",
      "\n",
      "This script processes the input string to build a matrix, transposes it, and prints the transposed matrix in the specified format.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Write a bash script that takes a matrix represented as a string with\n",
    "format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    # reasoning_effort=\"medium\",  # optional\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "financeRAG_acm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
